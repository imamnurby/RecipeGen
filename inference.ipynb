{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebacff7-dc38-40c3-abc8-f99dcf15410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9dcce-1d05-46dc-979c-a91378a8237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForCausalLM, EncoderDecoderModel)\n",
    "\n",
    "from models import CustomEncoderDecoderModel\n",
    "from data_collator import DataCollatorForSeq2Seq\n",
    "from trainer import CustomTrainer, CustomTrainingArguments\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from typing import Optional, Any, Union, List, Dict, Tuple\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef85cba-cbe8-4962-b813-8542b9f293f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036f29b-88e7-4369-a3f5-3d0ef81d29d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## experiment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a92413-aba3-48d5-9fee-b792f88f2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"set random seed.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(4321)\n",
    "\n",
    "DECODER_CLASSES = {'roberta-base': (RobertaForCausalLM, RobertaConfig)}\n",
    "# DATASET_PATH = \"dataset-ifttt-zenodo\"\n",
    "DATASET_PATH = \"dataset-original\"\n",
    "os.path.exists(DATASET_PATH)\n",
    "\n",
    "# specify pretrained model\n",
    "MODEL = \"roberta\"\n",
    "assert(MODEL in ('roberta', 'codebert'))\n",
    "\n",
    "# specify training data\n",
    "EXPERIMENT = \"merged-prefix-ch-fc-field-interactive\"\n",
    "assert(EXPERIMENT in ('merged-prefix-ch-fc-field-oneshot', 'merged-prefix-ch-fc-field-interactive'))\n",
    "\n",
    "OUTPUT_DIR = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022\"\n",
    "\n",
    "LOAD_FROM_CKPT = True\n",
    "if LOAD_FROM_CKPT:\n",
    "    ckpt = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050\"\n",
    "    # assert(os.path.exists(ckpt) == True)\n",
    "\n",
    "DEBUG = None\n",
    "DATA_NUM = 128 if DEBUG else None\n",
    "NUM_BEAMS = 10\n",
    "RETURN_TOP_K = 10\n",
    "\n",
    "# setting for the tokenizer\n",
    "MAX_INPUT_LENGTH = 250 \n",
    "MAX_TARGET_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399754b-a5c0-4f30-8c49-cff88d427866",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = CustomTrainingArguments(\n",
    "    f\"{OUTPUT_DIR}\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    # save_steps=5000 if not DEBUG else 1,\n",
    "    # logging_steps=500 if not DEBUG else 1,\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3 if not DEBUG else 3,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    optim='adamw_torch',\n",
    "    generation_num_beams=NUM_BEAMS if NUM_BEAMS else None,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    num_return_sequences=RETURN_TOP_K,\n",
    "    metrics_to_check=[('eval_bleu_em', True)],\n",
    "    no_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab4273-c296-4029-8d82-94789718d798",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529d1b0-3f7a-4f10-8b96-fcfaef8e7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_path(root=DATASET_PATH, exp=EXPERIMENT):\n",
    "    \n",
    "    def helper_prepare_data(x, mode):\n",
    "        temp_list = x.split(\"<sep>\")\n",
    "        temp_list = [item.strip() for item in temp_list]\n",
    "\n",
    "        if mode==\"ac\":\n",
    "            temp_list = temp_list[:2].copy()\n",
    "        elif mode==\"af\":\n",
    "            temp_list = temp_list[:3].copy()\n",
    "\n",
    "        return \" \".join(temp_list)\n",
    "    \n",
    "    datapath = os.path.join(root, \"processed.csv\")\n",
    "    df = pd.read_csv(datapath)\n",
    "    if exp == \"merged-prefix-ch-fc-field-oneshot\":\n",
    "        prefix_ch=\"GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> \"\n",
    "        prefix_fc=\"GENERATE CHANNEL AND FUNCTION FOR BOTH TRIGGER AND ACTION <pf> \"\n",
    "        prefix_fd=\"GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> \"\n",
    "        function=df[df.granularity==\"function\"].copy()\n",
    "        function[\"source\"] = function.source.apply(lambda x: prefix_fc + x)\n",
    "        \n",
    "        channel=df[df.granularity==\"channel\"].copy()\n",
    "        channel[\"source\"] = channel.source.apply(lambda x: prefix_ch + x)\n",
    "        \n",
    "        field=df[df.granularity==\"field\"].copy()\n",
    "        field[\"source\"] = field.source.apply(lambda x: prefix_fd + x)\n",
    "        \n",
    "        df = pd.concat([channel, function, field])\n",
    "        # df.drop(columns=[\"granularity\"], inplace=True)\n",
    "        \n",
    "        df_dict={\n",
    "                # 'train_all': df[df.split=='train'].copy(),\n",
    "                # 'val_all': df[df.split=='val'].copy(), \n",
    "                'gold_all': df[df.split=='gold'].copy(),\n",
    "                'noisy_all': df[df.split=='noisy'].copy(),\n",
    "                'gold_function': df[(df.split=='gold') & (df.granularity=='function')].copy(),\n",
    "                'gold_channel': df[(df.split=='gold') & (df.granularity=='channel')].copy(),\n",
    "                'gold_field': df[(df.split=='gold') & (df.granularity=='field')].copy(),\n",
    "                'noisy_function': df[(df.split=='noisy') & (df.granularity=='function')].copy(),\n",
    "                'noisy_channel': df[(df.split=='noisy') & (df.granularity=='channel')].copy(),\n",
    "                'noisy_field': df[(df.split=='noisy') & (df.granularity=='field')].copy()}\n",
    "        \n",
    "    elif exp == \"merged-prefix-ch-fc-field-interactive\":\n",
    "        prefix_tc=\"GENERATE TRIGGER CHANNEL <pf> \"\n",
    "        prefix_tf=\"GENERATE TRIGGER FUNCTION <pf> \"\n",
    "        prefix_ac=\"GENERATE ACTION CHANNEL <pf> \"\n",
    "        prefix_af=\"GENERATE ACTION FUNCTION <pf> \"\n",
    "        df = df[df.granularity==\"function\"].copy()\n",
    "        \n",
    "        function_tc = df.copy()\n",
    "        function_tc['target'] = function_tc.target.apply(lambda x: x.split(\"<sep>\")[0].strip())\n",
    "        function_tc['source'] = function_tc.source.apply(lambda x: prefix_tc + x)\n",
    "        \n",
    "        function_tf = df.copy()\n",
    "        function_tf['temp'] = function_tf.target.apply(lambda x: x.split(\"<sep>\")[0].strip())\n",
    "        function_tf['target'] = function_tf.target.apply(lambda x: x.split(\"<sep>\")[1].strip())\n",
    "        function_tf['source'] = function_tf.apply(lambda x: prefix_tf + x.source + \" <out> \" + x.temp, axis=1)\n",
    "        function_tf.drop(columns=[\"temp\"], inplace=True)\n",
    "        \n",
    "        function_ac = df.copy()\n",
    "        function_ac['temp'] = function_ac.target.apply(lambda x: helper_prepare_data(x, mode=\"ac\"))\n",
    "        function_ac['target'] = function_ac.target.apply(lambda x: x.split(\"<sep>\")[2].strip())\n",
    "        function_ac['source'] = function_ac.apply(lambda x: prefix_ac + x.source + \" <out> \" + x.temp, axis=1)\n",
    "        function_ac.drop(columns=[\"temp\"], inplace=True)\n",
    "        \n",
    "        function_af = df.copy()\n",
    "        function_af['temp'] = function_af.target.apply(lambda x: helper_prepare_data(x, mode=\"af\"))\n",
    "        function_af['target'] = function_af.target.apply(lambda x: x.split(\"<sep>\")[3].strip())\n",
    "        function_af['source'] = function_af.apply(lambda x: prefix_af + x.source + \" <out> \" + x.temp, axis=1)\n",
    "        function_af.drop(columns=[\"temp\"], inplace=True)\n",
    "        \n",
    "        df = pd.concat([function_tc, function_tf, function_ac, function_af])\n",
    "        \n",
    "        df_dict={\n",
    "                # 'train_all': df[df.split=='train'].copy(),\n",
    "                # 'val_all': df[df.split=='val'].copy(), \n",
    "                'gold_all': df[df.split=='gold'].copy(),\n",
    "                'noisy_all': df[df.split=='noisy'].copy(),\n",
    "                'gold_tc': function_tc[(function_tc.split=='gold')].copy(),\n",
    "                'gold_tf': function_tf[(function_tf.split=='gold')].copy(),\n",
    "                'gold_ac': function_ac[(function_ac.split=='gold')].copy(),\n",
    "                'gold_af': function_af[(function_af.split=='gold')].copy(),\n",
    "                'noisy_tc': function_tc[(function_tc.split=='noisy')].copy(),\n",
    "                'noisy_tf': function_tf[(function_tf.split=='noisy')].copy(),\n",
    "                'noisy_ac': function_ac[(function_ac.split=='noisy')].copy(),\n",
    "                'noisy_af': function_af[(function_af.split=='noisy')].copy()}\n",
    "    return df_dict\n",
    "df_dict = get_dataset_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a90302-bdcb-450c-a14b-43549947f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dict['gold_af'].sample(n=10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e6daa-55fb-40b5-9634-416caafd13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(exp=EXPERIMENT, df_dict=df_dict):\n",
    "    if exp == \"merged-prefix-ch-fc-field-oneshot\":\n",
    "        # train_all = Dataset.from_pandas(df_dict['train_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        # val_all = Dataset.from_pandas(df_dict['val_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_all = Dataset.from_pandas(df_dict['gold_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_all = Dataset.from_pandas(df_dict['noisy_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_function = Dataset.from_pandas(df_dict['gold_function']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_channel = Dataset.from_pandas(df_dict['gold_channel']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_field = Dataset.from_pandas(df_dict['gold_field']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_function = Dataset.from_pandas(df_dict['noisy_function']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_channel = Dataset.from_pandas(df_dict['noisy_channel']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_field = Dataset.from_pandas(df_dict['noisy_field']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "\n",
    "        return DatasetDict({\n",
    "                            # 'train_all':train_all,\n",
    "                            # 'val_all':val_all,\n",
    "                            'gold_all':gold_all,\n",
    "                            'noisy_all':noisy_all,\n",
    "                            'gold_function': gold_function,\n",
    "                            'gold_channel': gold_channel,\n",
    "                            'gold_field': gold_field,\n",
    "                            'noisy_function': noisy_function,\n",
    "                            'noisy_channel': noisy_channel,\n",
    "                            'noisy_field': noisy_field\n",
    "                           })\n",
    "    elif exp == \"merged-prefix-ch-fc-field-interactive\":\n",
    "        # train_all = Dataset.from_pandas(df_dict['train_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        # val_all = Dataset.from_pandas(df_dict['val_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_all = Dataset.from_pandas(df_dict['gold_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_all = Dataset.from_pandas(df_dict['noisy_all']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_tc = Dataset.from_pandas(df_dict['gold_tc']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_tf = Dataset.from_pandas(df_dict['gold_tf']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_ac = Dataset.from_pandas(df_dict['gold_ac']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        gold_af = Dataset.from_pandas(df_dict['gold_af']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_tc = Dataset.from_pandas(df_dict['noisy_tc']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_tf = Dataset.from_pandas(df_dict['noisy_tf']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_ac = Dataset.from_pandas(df_dict['noisy_ac']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        noisy_af = Dataset.from_pandas(df_dict['noisy_af']).remove_columns(['__index_level_0__', 'split', 'granularity'])\n",
    "        \n",
    "        return DatasetDict({\n",
    "                            # 'train_all':train_all,\n",
    "                            # 'val_all':val_all,\n",
    "                            'gold_all':gold_all,\n",
    "                            'noisy_all':noisy_all,\n",
    "                            'gold_tc': gold_tc,\n",
    "                            'gold_tf': gold_tf,\n",
    "                            'gold_ac': gold_ac,\n",
    "                            'gold_af': gold_af,\n",
    "                            'noisy_tc': noisy_tc,\n",
    "                            'noisy_tf': noisy_tf,\n",
    "                            'noisy_ac': noisy_ac,\n",
    "                            'noisy_af': noisy_af\n",
    "                           })\n",
    "\n",
    "dataset = convert_to_dataset()\n",
    "\n",
    "print(dataset.column_names)\n",
    "print([dataset['noisy_af'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33513e-304b-4709-aef0-98f9733df4de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e51e9-8684-4b10-8994-21143b7eb8a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model=MODEL):\n",
    "    if LOAD_FROM_CKPT:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(ckpt)\n",
    "    else:\n",
    "        if model == 'roberta':\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        elif model == 'codebert':\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        else:\n",
    "            raise ValueError(f\"Undefined model type\")\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268058e0-e2b2-4eb3-99be-3447c78bf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=False)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=False)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"gold_all\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "for item in tokenized_datasets['noisy_all'][:8]['input_ids']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba289c8-e5e4-4716-b5f2-de1eb443be28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b2add-5b03-4f6c-86d3-8d1151f37186",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_FROM_CKPT:\n",
    "    model = EncoderDecoderModel.from_pretrained(ckpt)\n",
    "    print(f\"Loading from {ckpt}\")\n",
    "else:\n",
    "    model = CustomEncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", random_decoder=True, model_dict=DECODER_CLASSES)\n",
    "    print(\"Loading not from checkpoint\")\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.architectures = \"EncoderDecoderModel\"\n",
    "model.config.max_length = MAX_TARGET_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054ac1b-f520-490b-b451-a73302005d82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03a13a-63ca-4dfc-8085-e3e09a3a1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "if DEBUG:\n",
    "    batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "    batch.keys()\n",
    "    print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2dbf6-d013-4b6c-9514-6ee7a41e0cba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890a71e-11e9-443c-ac16-c4b0b90b4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not actually used, but still needed because the behaviour of the trainer is weird without this\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    def decode_preds(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        # In case the model returns more than the prediction logits\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100s in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds = [pred.split(\"<pf>\")[-1].strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.split(\"<pf>\")[-1].strip()] for label in decoded_labels]\n",
    "        return decoded_preds, decoded_labels\n",
    "    \n",
    "    decoded_preds, decoded_labels = decode_preds(eval_preds)\n",
    "    \n",
    "    bleu_dict = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # decoded_preds = [pred[0] for pred in decoded_preds]\n",
    "    decoded_labels = [label[0] for label in decoded_labels]\n",
    "    em_dict = em.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_dict[\"score\"],\n",
    "           \"em\": em_dict['exact_match'],\n",
    "           \"bleu_em\": (bleu_dict['score']+em_dict['exact_match'])/2}\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397d987-55a6-4452-b3d6-96d4ac73d63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"gold_all\"],\n",
    "    eval_dataset=tokenized_datasets[\"noisy_all\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d09b8-9550-4e35-b0c2-c8f823b883dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_tc\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['gold_tc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1925a-62de-4437-b17c-7c180f7b9775",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_tf\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['gold_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bdf6e-2f4d-40a5-9c1b-a33906ed63b0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_ac\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['gold_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d150094-682b-4e95-bece-2f377270dceb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_af\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['gold_af'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f89acb-bcea-48bc-b757-22e30545cff0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_tc\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['noisy_tc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e784dd-bd62-497d-8ebd-67ed6f53c3c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_tf\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['noisy_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a7c2d-b6e8-4f87-8cb9-9c08e2086fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_ac\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['noisy_ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd08f9-877c-464e-aa6d-863f178a16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_inference=f\"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_af\"\n",
    "trainer.inference(output_dir_inference=output_dir_inference, \n",
    "                 eval_dataset=tokenized_datasets['noisy_af'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9df2a-b3b1-4ca9-878c-d5d387c17153",
   "metadata": {},
   "source": [
    "# compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54164a-6ded-4d1a-94ab-65e685d8613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"sacrebleu\")\n",
    "em = load_metric(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1211d7f-865b-4693-b284-100a844cc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(path):\n",
    "    preds = []\n",
    "    refs = []\n",
    "    files = os.listdir(path)\n",
    "    files.sort(key=natural_keys)\n",
    "    for item in files:\n",
    "        temp_path = os.path.join(path, item)\n",
    "        if os.path.isfile(temp_path):\n",
    "            with open(temp_path, \"r\") as f:\n",
    "                temp_list = f.readlines()\n",
    "                temp_list = [x.strip() for x in temp_list]\n",
    "            if item.endswith(\".pred\"):\n",
    "                preds.append(temp_list)\n",
    "            elif item.endswith(\".gold\"):\n",
    "                refs.append(temp_list)\n",
    "    return preds, refs\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "def compute_metrics(preds, refs):\n",
    "    def compute_mrr(preds, refs, top_k):\n",
    "        temp_preds = [x[:top_k] for x in preds]\n",
    "        temp_refs = [x[:top_k] for x in refs]\n",
    "        mrr_k = np.array(temp_preds) == np.array(refs)\n",
    "        mrr_k = mrr_k.astype(\"int\").tolist()\n",
    "        mrr_k = (np.asarray(r).nonzero()[0] for r in mrr_k)\n",
    "        mrr_k = np.mean([1. / (r[0] + 1) if r.size else 0. for r in mrr_k])\n",
    "        return round(mrr_k, 3)\n",
    "    \n",
    "    def prepare_for_bleu(input_list):\n",
    "        input_list = [item[0] for item in input_list]\n",
    "        input_list = [item.split(\"<sep>\") for item in input_list]\n",
    "        output_list = []\n",
    "        for item in input_list:\n",
    "            temp_list = []\n",
    "            for subitem in item:\n",
    "                temp_list.append(subitem.strip())\n",
    "            output_list.append(temp_list)\n",
    "        output_list = [' '.join(item) for item in output_list]\n",
    "        return output_list\n",
    "    \n",
    "    mrr_3 = compute_mrr(preds, refs, 3)\n",
    "    mrr_5 = compute_mrr(preds, refs, 5)\n",
    "    mrr_10 = compute_mrr(preds, refs, 10)\n",
    "    \n",
    "    preds_bleu = prepare_for_bleu(preds)\n",
    "    refs_bleu = prepare_for_bleu(refs)\n",
    "    refs_bleu = [[x] for x in refs_bleu]\n",
    "    bleu_dict = bleu.compute(predictions=preds_bleu, references=refs_bleu)\n",
    "    \n",
    "    preds = [pred[0] for pred in preds]\n",
    "    refs = [label[0] for label in refs]\n",
    "    em_dict = em.compute(predictions=preds, references=refs)\n",
    "    return {\"bleu\": round(bleu_dict[\"score\"], 3),\n",
    "           \"em\": round(em_dict['exact_match'], 3),\n",
    "           \"bleu_em\": round((bleu_dict['score']+em_dict['exact_match'])/2, 3),\n",
    "           \"mrr_3\": mrr_3,\n",
    "           \"mrr_5\": mrr_5,\n",
    "           \"mrr_10\": mrr_10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bd043-4e91-4b9e-a602-09bbc6459502",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_tc\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268619e-1a20-4719-9090-ff3948462294",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7f982-025c-4b33-8e65-bd7cd80ab695",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_tf\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f14c0-1c0b-442a-aae9-2734bd47122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39388628-07cd-44f3-8641-4cc2ff0e2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_ac\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb63643-58c5-4360-86a2-46ce8a50a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38186802-287b-4554-9906-67f4aa9ea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_gold_af\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cc467-d630-40b7-9914-a624678c8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddd8b5-4e2f-4b0a-b1d8-6dec4c1c4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_tc\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c2158-53f4-4a49-9cb4-976eecbc3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffa0e8-3481-4a03-90b4-e05a32e59c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_tf\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c511d1-fcb1-4bcb-83f7-4d3a14620e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d412e-2c63-4a2f-84d6-88bc35634191",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_ac\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a02f7b-1d3e-4e38-ab15-32546ed8db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8f297-e9e5-4e1d-be94-08cb5272f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/rob2rand_merged_w_prefix_interactive_5-6-2022/checkpoint-427050/interactive_noisy_af\"\n",
    "preds, refs = get_predictions(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bf361-98bc-4a3e-915f-216a0bdde889",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(preds, refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbdb52-ebac-4fa5-8697-e83bb9aa7001",
   "metadata": {
    "tags": []
   },
   "source": [
    "# push model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c4f45-f29b-4c94-813b-de0664b78362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95257f-1d33-42a5-b3af-bbf0a2d2df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967c978-9e42-45ea-9276-ab7ab97238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.output_dir = \"rob2rand_merged_w_prefix_c_fc_interactive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539eb21-d4f0-4bd9-aabf-9e7e75082005",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e697d9-d1b1-4861-b487-ea61213fa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f779703-f54a-48cc-a272-93e4c7967034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
