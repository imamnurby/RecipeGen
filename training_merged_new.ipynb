{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef9dcce-1d05-46dc-979c-a91378a8237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForCausalLM, EncoderDecoderModel)\n",
    "\n",
    "from models import CustomEncoderDecoderModel\n",
    "from data_collator import DataCollatorForSeq2Seq\n",
    "from trainer import CustomTrainer, CustomTrainingArguments\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from typing import Optional, Any, Union, List, Dict, Tuple\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0a761a-f907-4b84-8b2b-8285a6ea3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef85cba-cbe8-4962-b813-8542b9f293f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036f29b-88e7-4369-a3f5-3d0ef81d29d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## experiment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a92413-aba3-48d5-9fee-b792f88f2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"set random seed.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(4321)\n",
    "\n",
    "DECODER_CLASSES = {'roberta-base': (RobertaForCausalLM, RobertaConfig)}\n",
    "# DATASET_PATH = \"dataset-ifttt-zenodo\"\n",
    "DATASET_PATH = \"dataset-original\"\n",
    "os.path.exists(DATASET_PATH)\n",
    "\n",
    "# specify pretrained model\n",
    "MODEL = \"roberta\"\n",
    "assert(MODEL in ('roberta', 'codebert'))\n",
    "\n",
    "# specify training data\n",
    "EXPERIMENT = \"merged-prefix-ch-fc-field\"\n",
    "assert(EXPERIMENT in ('chen', 'mi', 'merged', 'chen-prefix', 'chen-prefix-ch', 'chen-prefix-fc',\n",
    "                     'merged-prefix-ch-fc-field'))\n",
    "\n",
    "OUTPUT_DIR = \"models/rob2rand_merged_w_prefix_2-6-22\"\n",
    "\n",
    "LOAD_FROM_CKPT = False\n",
    "if LOAD_FROM_CKPT:\n",
    "    ckpt = \"models/rob2rand_chen_w_prefix_26-5-22/checkpoint-70000\"\n",
    "    # assert(os.path.exists(ckpt) == True)\n",
    "\n",
    "DEBUG = None\n",
    "DATA_NUM = 128 if DEBUG else None\n",
    "NUM_BEAMS = 3\n",
    "RETURN_TOP_K = 1\n",
    "\n",
    "# setting for the tokenizer\n",
    "MAX_INPUT_LENGTH = 100 \n",
    "MAX_TARGET_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8399754b-a5c0-4f30-8c49-cff88d427866",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = CustomTrainingArguments(\n",
    "    f\"{OUTPUT_DIR}\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000 if not DEBUG else 1,\n",
    "    logging_steps=500 if not DEBUG else 1,\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3 if not DEBUG else 3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    optim='adamw_torch',\n",
    "    generation_num_beams=NUM_BEAMS if NUM_BEAMS else None,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    num_return_sequences=RETURN_TOP_K,\n",
    "    metrics_to_check=[('eval_bleu_em', True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab4273-c296-4029-8d82-94789718d798",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4529d1b0-3f7a-4f10-8b96-fcfaef8e7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f960f6d82a4032b639143fc712f7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baccb61804d40579251922351c44ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc73ebfbc9e47c9a975dfe11d1f86c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_path(root=DATASET_PATH, exp=EXPERIMENT):\n",
    "    prefix_ch=\"GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> \"\n",
    "    prefix_fc=\"GENERATE CHANNEL AND FUNCTION FOR BOTH TRIGGER AND ACTION <pf> \"\n",
    "    prefix_fd=\"GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> \"\n",
    "    if exp == \"merged-prefix-ch-fc-field\":\n",
    "        datapath = os.path.join(root, \"processed.csv\")\n",
    "        df = pd.read_csv(datapath)\n",
    "        function=df[df.granularity==\"function\"].copy()\n",
    "        function[\"source\"] = function.source.progress_apply(lambda x: prefix_fc + x)\n",
    "        \n",
    "        channel=df[df.granularity==\"channel\"].copy()\n",
    "        channel[\"source\"] = channel.source.progress_apply(lambda x: prefix_ch + x)\n",
    "        \n",
    "        field=df[df.granularity==\"field\"].copy()\n",
    "        field[\"source\"] = field.source.progress_apply(lambda x: prefix_fd + x)\n",
    "        \n",
    "        df = pd.concat([channel, function, field])\n",
    "        df.drop(columns=[\"granularity\"], inplace=True)\n",
    "        \n",
    "        df_dict={'train': df[df.split=='train'].copy(),\n",
    "                'val': df[df.split=='val'].copy(), \n",
    "                'gold': df[df.split=='gold'].copy(),\n",
    "                'noisy': df[df.split=='noisy'].copy()}\n",
    "    return df_dict\n",
    "df_dict = get_dataset_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a90302-bdcb-450c-a14b-43549947f91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138106</th>\n",
       "      <td>GENERATE CHANNEL ONLY WITHOUT FUNCTION &lt;pf&gt; ne...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>Gmail &lt;sep&gt; Google_Calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138107</th>\n",
       "      <td>GENERATE CHANNEL ONLY WITHOUT FUNCTION &lt;pf&gt; ma...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>Weather_Underground &lt;sep&gt; Gmail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138108</th>\n",
       "      <td>GENERATE CHANNEL ONLY WITHOUT FUNCTION &lt;pf&gt; te...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>Weather_Underground &lt;sep&gt; SMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138109</th>\n",
       "      <td>GENERATE CHANNEL ONLY WITHOUT FUNCTION &lt;pf&gt; te...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>Weather_Underground &lt;sep&gt; SMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138110</th>\n",
       "      <td>GENERATE CHANNEL ONLY WITHOUT FUNCTION &lt;pf&gt; if...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>Weather_Underground &lt;sep&gt; Gmail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416137</th>\n",
       "      <td>GENERATE ON THE FIELD-LEVEL GRANULARITY &lt;pf&gt; w...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>RSS_Feed &lt;sep&gt; RSS_Feed.New_feed_item &lt;sep&gt; Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416138</th>\n",
       "      <td>GENERATE ON THE FIELD-LEVEL GRANULARITY &lt;pf&gt; n...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>iOS_Contacts &lt;sep&gt; iOS_Contacts.Any_new_contac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416139</th>\n",
       "      <td>GENERATE ON THE FIELD-LEVEL GRANULARITY &lt;pf&gt; n...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>RSS_Feed &lt;sep&gt; RSS_Feed.New_feed_item &lt;sep&gt; Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416140</th>\n",
       "      <td>GENERATE ON THE FIELD-LEVEL GRANULARITY &lt;pf&gt; n...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>RSS_Feed &lt;sep&gt; RSS_Feed.New_feed_item &lt;sep&gt; Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416141</th>\n",
       "      <td>GENERATE ON THE FIELD-LEVEL GRANULARITY &lt;pf&gt; t...</td>\n",
       "      <td>noisy</td>\n",
       "      <td>iOS_Reminders &lt;sep&gt; iOS_Reminders.New_reminder...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1824 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   source  split  \\\n",
       "138106  GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> ne...  noisy   \n",
       "138107  GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> ma...  noisy   \n",
       "138108  GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> te...  noisy   \n",
       "138109  GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> te...  noisy   \n",
       "138110  GENERATE CHANNEL ONLY WITHOUT FUNCTION <pf> if...  noisy   \n",
       "...                                                   ...    ...   \n",
       "416137  GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> w...  noisy   \n",
       "416138  GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> n...  noisy   \n",
       "416139  GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> n...  noisy   \n",
       "416140  GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> n...  noisy   \n",
       "416141  GENERATE ON THE FIELD-LEVEL GRANULARITY <pf> t...  noisy   \n",
       "\n",
       "                                                   target  \n",
       "138106                        Gmail <sep> Google_Calendar  \n",
       "138107                    Weather_Underground <sep> Gmail  \n",
       "138108                      Weather_Underground <sep> SMS  \n",
       "138109                      Weather_Underground <sep> SMS  \n",
       "138110                    Weather_Underground <sep> Gmail  \n",
       "...                                                   ...  \n",
       "416137  RSS_Feed <sep> RSS_Feed.New_feed_item <sep> Fe...  \n",
       "416138  iOS_Contacts <sep> iOS_Contacts.Any_new_contac...  \n",
       "416139  RSS_Feed <sep> RSS_Feed.New_feed_item <sep> Fe...  \n",
       "416140  RSS_Feed <sep> RSS_Feed.New_feed_item <sep> Fe...  \n",
       "416141  iOS_Reminders <sep> iOS_Reminders.New_reminder...  \n",
       "\n",
       "[1824 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['noisy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b8bf5a-cf5a-499c-aead-a3e00e3c91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(path_dict=path_dict, number=None):\n",
    "#     assert(type(path_dict)==dict)\n",
    "#     df_dict = {}\n",
    "#     for split, path in path_dict.items():\n",
    "#         if number:\n",
    "#             df_dict[split] = pd.read_pickle(path).sample(n=number, random_state=1234).copy()\n",
    "#         else:\n",
    "#             df_dict[split] = pd.read_pickle(path)\n",
    "#     return df_dict\n",
    "\n",
    "# if DATA_NUM:\n",
    "#     df_dict = load_dataset(number=DATA_NUM)\n",
    "# else:\n",
    "#     df_dict = load_dataset()\n",
    "\n",
    "# df_dict['train'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6220f612-23fe-4744-9b08-1f9869780840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dict['train'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37e6daa-55fb-40b5-9634-416caafd13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(df_dict=df_dict):\n",
    "    train = Dataset.from_pandas(df_dict['train']).remove_columns(['__index_level_0__', 'split'])\n",
    "    val = Dataset.from_pandas(df_dict['val']).remove_columns(['__index_level_0__', 'split'])\n",
    "    gold = Dataset.from_pandas(df_dict['gold']).remove_columns(['__index_level_0__', 'split'])\n",
    "    noisy = Dataset.from_pandas(df_dict['noisy']).remove_columns(['__index_level_0__', 'split'])\n",
    "    \n",
    "    return DatasetDict({'train':train,\n",
    "                        'val':val,\n",
    "                        'gold':gold,\n",
    "                        'noisy':noisy})\n",
    "\n",
    "dataset = convert_to_dataset()\n",
    "\n",
    "if DEBUG:\n",
    "    print(dataset.column_names)\n",
    "    print([dataset['train'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33513e-304b-4709-aef0-98f9733df4de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349e51e9-8684-4b10-8994-21143b7eb8a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model=MODEL):\n",
    "    if LOAD_FROM_CKPT:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(ckpt)\n",
    "    else:\n",
    "        if model == 'roberta':\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        elif model == 'codebert':\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        else:\n",
    "            raise ValueError(f\"Undefined model type\")\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268058e0-e2b2-4eb3-99be-3447c78bf3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5391c33e0e6426ab0bbf86b23a91c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/410 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02cfc321f844251ab9d9d4525c138b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7215ff5c047b47cb9c80a2dbc0bdb2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e5159e931a4bd289419b849675730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=False)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=False)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    for item in tokenized_datasets['train'][:8]['input_ids']:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba289c8-e5e4-4716-b5f2-de1eb443be28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63b2add-5b03-4f6c-86d3-8d1151f37186",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading not from checkpoint\n"
     ]
    }
   ],
   "source": [
    "if LOAD_FROM_CKPT:\n",
    "    model = EncoderDecoderModel.from_pretrained(ckpt)\n",
    "    print(f\"Loading from {ckpt}\")\n",
    "else:\n",
    "    model = CustomEncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", random_decoder=True, model_dict=DECODER_CLASSES)\n",
    "    print(\"Loading not from checkpoint\")\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.architectures = \"EncoderDecoderModel\"\n",
    "model.config.max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1054ac1b-f520-490b-b451-a73302005d82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf03a13a-63ca-4dfc-8085-e3e09a3a1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "if DEBUG:\n",
    "    batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "    batch.keys()\n",
    "    print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4d7f8-2db2-4360-8312-1a9cec25f420",
   "metadata": {
    "tags": []
   },
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e03afeb-663d-4f0a-9db3-c1ca4a6769fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"sacrebleu\")\n",
    "em = load_metric(\"exact_match\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    def decode_preds(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        # In case the model returns more than the prediction logits\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100s in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds = [pred.split(\"<pf>\")[-1].strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.split(\"<pf>\")[-1].strip()] for label in decoded_labels]\n",
    "        return decoded_preds, decoded_labels\n",
    "    \n",
    "    decoded_preds, decoded_labels = decode_preds(eval_preds)\n",
    "    \n",
    "    bleu_dict = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # decoded_preds = [pred[0] for pred in decoded_preds]\n",
    "    decoded_labels = [label[0] for label in decoded_labels]\n",
    "    em_dict = em.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_dict[\"score\"],\n",
    "           \"em\": em_dict['exact_match'],\n",
    "           \"bleu_em\": (bleu_dict['score']+em_dict['exact_match'])/2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2dbf6-d013-4b6c-9514-6ee7a41e0cba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# custom trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3397d987-55a6-4452-b3d6-96d4ac73d63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd423d5f-ba10-474c-b47e-dc2402562310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trainer(output_dir, split=None, suffix=None):\n",
    "    assert(split in tokenized_datasets)\n",
    "    res = trainer.evaluate(eval_dataset=tokenized_datasets[split])\n",
    "    print(res)\n",
    "    \n",
    "    filename = f\"{output_dir}/metrics_{split}.txt\" if suffix==None else f\"{output_dir}/metrics_{split}_{suffix}.txt\" \n",
    "    with open(filename, \"a\") as f:\n",
    "        json.dump(res, f)\n",
    "        f.write(\"\\n\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed19b0e9-d13d-4d69-95cb-a29770c5eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = evaluate_trainer(output_dir=f\"{ckpt}\",\n",
    "                       # split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de4416e-bbb0-4da6-832f-71eba27486c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2001' max='19218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2001/19218 17:43 < 2:32:41, 1.88 it/s, Epoch 0.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Em</th>\n",
       "      <th>Bleu Em</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.315000</td>\n",
       "      <td>3.639773</td>\n",
       "      <td>15.265869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.632935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.998600</td>\n",
       "      <td>1.724522</td>\n",
       "      <td>20.121760</td>\n",
       "      <td>2.652150</td>\n",
       "      <td>11.386955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.625200</td>\n",
       "      <td>0.941548</td>\n",
       "      <td>52.124041</td>\n",
       "      <td>15.047460</td>\n",
       "      <td>33.585750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/56 00:35 < 00:18, 1.01 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em\n",
      "Configuration saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/config.json\n",
      "Model weights saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/pytorch_model.bin\n",
      "tokenizer config file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/tokenizer_config.json\n",
      "Special tokens file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em\n",
      "Configuration saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/config.json\n",
      "Model weights saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/pytorch_model.bin\n",
      "tokenizer config file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/tokenizer_config.json\n",
      "Special tokens file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em\n",
      "Configuration saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/config.json\n",
      "Model weights saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/pytorch_model.bin\n",
      "tokenizer config file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/tokenizer_config.json\n",
      "Special tokens file saved in models/rob2rand_merged_w_prefix_2-6-22/best_eval_bleu_em/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/ifttt/trainer.py:843\u001b[0m, in \u001b[0;36mCustomTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m--> 843\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1622\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 1624\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/data/ifttt/trainer.py:953\u001b[0m, in \u001b[0;36mCustomTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[1;32m    950\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    952\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m--> 953\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m    964\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    965\u001b[0m     speed_metrics(\n\u001b[1;32m    966\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m     )\n\u001b[1;32m    971\u001b[0m )\n",
      "File \u001b[0;32m/data/ifttt/trainer.py:1098\u001b[0m, in \u001b[0;36mCustomTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1101\u001b[0m     xm\u001b[38;5;241m.\u001b[39mmark_step()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer_seq2seq.py:177\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmain_input_name]\n\u001b[0;32m--> 177\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# in case the batch is shorter than max length, the output should be padded\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1315\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1312\u001b[0m         input_ids, expand_size\u001b[38;5;241m=\u001b[39mnum_beams, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1313\u001b[0m     )\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1331\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k, top_p\u001b[38;5;241m=\u001b[39mtop_p, typical_p\u001b[38;5;241m=\u001b[39mtypical_p, temperature\u001b[38;5;241m=\u001b[39mtemperature, num_beams\u001b[38;5;241m=\u001b[39mnum_beams\n\u001b[1;32m   1332\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:2210\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[0;32m-> 2210\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2219\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2220\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_beam_search.py:253\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id)\u001b[0m\n\u001b[1;32m    251\u001b[0m batch_beam_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size \u001b[38;5;241m+\u001b[39m next_index\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# add to generated hypotheses if end of sentence\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m eos_token_id):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# if beam_token does not belong to top num_beams tokens, it should not be added\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     is_beam_token_worse_than_top_num_beams \u001b[38;5;241m=\u001b[39m beam_token_rank \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_beam_token_worse_than_top_num_beams:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4c1925a-62de-4437-b17c-7c180f7b9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 06:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1106535941362381, 'eval_bleu': 80.44846791315841, 'eval_em': 48.80829015544042, 'eval_bleu_em': 64.62837903429941, 'eval_runtime': 77.5655, 'eval_samples_per_second': 37.323, 'eval_steps_per_second': 0.593}\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_trainer(split='val',\n",
    "                       output_dir=ckpt,\n",
    "                       suffix=\"fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef4bdf6e-2f4d-40a5-9c1b-a33906ed63b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009913727641105652, 'eval_bleu': 97.71500511168604, 'eval_em': 90.49180327868852, 'eval_bleu_em': 94.10340419518728, 'eval_runtime': 8.2387, 'eval_samples_per_second': 37.02, 'eval_steps_per_second': 0.607}\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_trainer(split='gold',\n",
    "                       output_dir=ckpt,\n",
    "                       suffix=\"fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d150094-682b-4e95-bece-2f377270dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09393603354692459, 'eval_bleu': 84.45299770854542, 'eval_em': 52.015604681404426, 'eval_bleu_em': 68.23430119497493, 'eval_runtime': 21.0016, 'eval_samples_per_second': 36.616, 'eval_steps_per_second': 0.619}\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_trainer(split='noisy',\n",
    "                       output_dir=ckpt,\n",
    "                       suffix=\"fc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbdb52-ebac-4fa5-8697-e83bb9aa7001",
   "metadata": {},
   "source": [
    "# push model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc8c4f45-f29b-4c94-813b-de0664b78362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d95257f-1d33-42a5-b3af-bbf0a2d2df74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d127a781b1174d0c9193916168c0c2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0967c978-9e42-45ea-9276-ab7ab97238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.output_dir = \"rob2rand_chen_w_prefix_c_fc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5539eb21-d4f0-4bd9-aabf-9e7e75082005",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderConfig {\n",
       "  \"_name_or_path\": \"models/rob2rand_chen_w_prefix_26-5-22/checkpoint-70000\",\n",
       "  \"architectures\": \"EncoderDecoderModel\",\n",
       "  \"decoder\": {\n",
       "    \"_name_or_path\": \"roberta-base\",\n",
       "    \"add_cross_attention\": true,\n",
       "    \"architectures\": [\n",
       "      \"RobertaForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": true,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 514,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"roberta\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"transformers_version\": \"4.18.0\",\n",
       "    \"type_vocab_size\": 1,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 50265\n",
       "  },\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"encoder\": {\n",
       "    \"_name_or_path\": \"roberta-base\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"RobertaForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 514,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"roberta\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 12,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"transformers_version\": \"4.18.0\",\n",
       "    \"type_vocab_size\": 1,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 50265\n",
       "  },\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"max_length\": 100,\n",
       "  \"model_type\": \"encoder-decoder\",\n",
       "  \"pad_token_id\": 1,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": null,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5e697d9-d1b1-4861-b487-ea61213fa801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/imamnurby/rob2rand_chen_w_prefix_c_fc into local empty directory.\n",
      "Saving model checkpoint to rob2rand_chen_w_prefix_c_fc\n",
      "Configuration saved in rob2rand_chen_w_prefix_c_fc/config.json\n",
      "Model weights saved in rob2rand_chen_w_prefix_c_fc/pytorch_model.bin\n",
      "tokenizer config file saved in rob2rand_chen_w_prefix_c_fc/tokenizer_config.json\n",
      "Special tokens file saved in rob2rand_chen_w_prefix_c_fc/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d77750badc4a8b8095632744c290a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Upload file pytorch_model.bin'), FloatProgress(value=0.0, max=1111040466.0), HTML(vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39fce3341b1420bb4af4f93306e2ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Upload file training_args.bin'), FloatProgress(value=0.0, max=3247.0), HTML(value='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/imamnurby/rob2rand_chen_w_prefix_c_fc\n",
      "   c8341d3..aa930dc  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/imamnurby/rob2rand_chen_w_prefix_c_fc\n",
      "   aa930dc..5910bc6  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/imamnurby/rob2rand_chen_w_prefix_c_fc/commit/aa930dc20280e00ff37eaaca6bd41a3b24b1417d'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f779703-f54a-48cc-a272-93e4c7967034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
